## Rocket Chat image version
## ref: https://hub.docker.com/r/rocketchat/rocket.chat/tags
##
image:
  repository: docker.io/rocketchat/rocket.chat
  tag: 3.14.5
  pullPolicy: IfNotPresent

## Host for the application
##
# host:

replicaCount: 1
minAvailable: 1

nameOverride: chatserver

smtp:
  enabled: false
  username:
  password:
  host:
  port: 587

# Extra env vars for Rocket.Chat:
extraEnv:  # |
  # - name: MONGO_OPTIONS
  #   value: '{"ssl": "true"}'
  # - name: MONGO_OPLOG_URL
  #   value: mongodb://oploguser:password@rocket-1:27017/local&replicaSet=rs0

## Pod anti-affinity can prevent the scheduler from placing RocketChat replicas on the same node.
## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
##
podAntiAffinity: ""

## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
##
podAntiAffinityTopologyKey: kubernetes.io/hostname

## Assign custom affinity rules to the RocketChat instance
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
##
affinity: {}
# nodeAffinity:
#   requiredDuringSchedulingIgnoredDuringExecution:
#     nodeSelectorTerms:
#     - matchExpressions:
#       - key: kubernetes.io/e2e-az-name
#         operator: In
#         values:
#         - e2e-az1
#         - e2e-az2

## MongoDB URL if using an externally provisioned MongoDB
externalMongodbUrl:  # mongodb://user:password@localhost:27017/rocketchat
externalMongodbOplogUrl:  # mongodb://user:password@localhost:27017/local?replicaSet=rs0&authSource=admin

##
## MongoDB chart configuration
### ref https://github.com/helm/charts/tree/master/stable/mongodb#configuration
##
mongodb:
  ## Enable or disable MongoDB dependency completely.
  enabled: true

  # auth.rootPassword:

  auth.username: rocketchat
  # auth.password:
  auth.database: rocketchat
  
  architecture: replicaset
  replicaCount: 1
  arbiter:
    enabled: false
    pdb:
      minAvailable: 0
  pdb:
    minAvailable: 0
  existingClaim: pvc-mongo-primary

   # key:

  persistence:
    enabled: true
    ## mongodb data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 8Gi

## Enable persistence using Persistent Volume Claims
## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
##
persistence:
  enabled: false
  # existingClaim: existingClaimName
  ## rocketchat data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  # storageClass: "-"
  accessMode: ReadWriteOnce
  size: 8Gi

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
# resources:
#   requests:
#     memory: 512Mi
#     cpu: 300m

securityContext:
  enabled: true
  runAsUser: 999
  fsGroup: 999

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true

  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

## Configure the ingress object to hook into existing infastructure
### ref : http://kubernetes.io/docs/user-guide/ingress/
###
ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: "nginx"
  path: /
  tls:
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

service:
  annotations: {}
  # service.beta.kubernetes.io/aws-load-balancer-internal: "0.0.0.0/0"

  labels: {}
  # key: value

  ## ServiceType
  ## ref: https://kubernetes.io/docs/user-guide/services/#publishing-services---service-types
  type: NodePort

  ## Optional static port assignment for service type NodePort.
  nodePort: 30000

  port: 3000

## Optional Pod Labels.
podLabels: {}

## Optional Pod Annotations.
podAnnotations: {}
  # prometheus.io/port: "9458"
  # prometheus.io/path: "/metrics"
  # prometheus.io/scrape: "true"

## Liveness and readiness probe values
## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
##
livenessProbe:
  enabled: true
  initialDelaySeconds: 60
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

readinessProbe:
  enabled: true
  initialDelaySeconds: 10
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3
  successThreshold: 1

redis:
  # Install should be `true` if the redis subchart should be used
  install: true
  # cluster settings for redis (Rasa does currently not support redis sentinels)
  cluster:
    # set up a single Redis instance, as `redis-py` does not support clusters (https://github.com/andymccurdy/redis-py#cluster-mode)
    enabled: false
  # redisPort: port which should be used to expose redis to the other components
  redisPort: 6379
  # existingSecret which should be used for the password instead of putting it in the values file
  existingSecret: ""
  # existingSecretPasswordKey is the key to get the password when an external redis instance is provided
  existingSecretPasswordKey: ""
  # existingHost is the host which is used when an external redis instance is provided (`install: false`)
  existingHost: ""
  # # security context for the redis container (please see the documentation of the subchart)
  # securityContext:
  #   enabled: true
  #   fsGroup: 1001
  #   runAsUser: 1001
